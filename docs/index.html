<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Synthesizing Visual Concepts as Vision-Language Programs">
  <meta name="keywords" content="Vision-Language Models, Program Synthesis, Neuro-Symbolic AI, Bongard">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="YyDLtcGtegAATTYnhSXnvL6klnX3Bk00jCY9dZQxCTo" />
  <title>Synthesizing Visual Concepts as Vision-Language Programs</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/aiml_logo.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://ml-research.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://ml-research.github.io/bongard-in-wonderland/">
              Bongard in Wonderland
            </a>
            <a class="navbar-item" href="https://ml-research.github.io/NeuralConceptBinder/">
              Neural Concept Binder
            </a>
            <a class="navbar-item" href="https://davste13.github.io/Object-Centric-Concept-Bottlenecks/">
              Object-Centric Concept-Bottlenecks
            </a>
            <a class="navbar-item" href="https://ml-research.github.io/pix2code/">
              Pix2Code
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>

  <section class="hero is-medium vlp-hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="has-text-centered">
          <h1 class="title is-1 publication-title">Synthesizing Visual Concepts as Vision-Language Programs</h1>
          <p class="subtitle is-4 mt-3">
            Vision-Language Programs pair VLM perception with symbolic program synthesis to produce executable,
            interpretable rules for few-shot visual reasoning.
          </p>
          <div class="is-size-5 publication-authors mt-4">
            <span class="author-block">
              <a href="https://ml-research.github.io/people/awuest/index.html" target="_blank">Antonia
                W&uuml;st</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://wolfstam.github.io/" target="_blank">Wolfgang Stammer</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://ml-research.github.io/people/hshindo/index.html" target="_blank">Hikaru
                Shindo</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://ml-research.github.io/people/lhelff/index.html" target="_blank">Lukas
                Helff</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/devendradhami" target="_blank">Devendra Singh
                Dhami</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://ml-research.github.io/people/kkersting/" target="_blank">Kristian
                Kersting</a><sup>1,3,5</sup>
            </span>
          </div>
          <div class="is-size-6 publication-authors mt-2">
            <span class="author-block"><sup>1</sup>AI/ML Lab, TU Darmstadt</span>
            <span class="author-block"><sup>2</sup>Max Planck Institute for Informatics</span>
            <span class="author-block"><sup>3</sup>Hessian Center for AI (hessian.AI)</span>
            <span class="author-block"><sup>4</sup>Uncertainty in AI Group, TU Eindhoven</span>
            <span class="author-block"><sup>5</sup>German Research Center for AI (DFKI)</span>
          </div>
          <div class="buttons is-centered mt-5">
            <a class="button is-dark is-rounded" href=https://arxiv.org/pdf/2511.18964" target="_blank">
              <span class="icon"><i class="fas fa-file-pdf"></i></span>
              <span>Paper (PDF)</span>
            </a>
            <a class="button is-dark is-rounded" href="https://arxiv.org/abs/2511.18964" target="_blank">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>arXiv</span>
            </a>
            <a class="button is-dark is-rounded" href="https://github.com/ml-research/vision-language-programs"
              target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-vcentered is-variable is-4">
          <div class="column has-text-centered">
            <img src="./static/images/motivation_5.webp" class="motivation-image" alt="Motivation figure" />
          </div>
          <div class="column">
            <h2 class="title is-3">Why Vision-Language Programs?</h2>
            <div class="content is-size-5">
              <p>VLMs excel at perception but fall apart on systematic visual reasoning, often hallucinating rules that
                violate constraints. VLP keeps VLMs for perception while delegating logic to program synthesis, so rules
                stay executable and auditable.</p>
              <ul>
                <li>Structured visual descriptions become neurosymbolic programs.</li>
                <li>Programs run directly on images, respecting positive/negative evidence.</li>
                <li>Transparent explanations make shortcut mitigation straightforward.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Vision-Language models achieve strong performance on multimodal tasks but often fail at systematic
                visual reasoning. We propose Vision-Language Programs (VLP), which combine the perceptual flexibility of
                VLMs with the structured reasoning of program synthesis. VLP asks a VLM to produce type-constrained
                visual descriptors, compiles them into executable programs, and searches for the most accurate,
                highest-likelihood rule under a probabilistic grammar. The resulting programs execute on images, remain
                consistent with task constraints, and provide human-interpretable explanations that enable shortcut
                mitigation. Experiments on synthetic and real-world datasets demonstrate that VLPs outperform direct and
                structured prompting, especially on tasks requiring complex logical reasoning.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-multiline is-variable is-6">
        <div class="column is-half">
          <h2 class="title is-4">Method at a glance</h2>
          <ul class="content">
            <li><strong>Symbol grounding:</strong> VLM proposes objects, properties, and actions for the task.</li>
            <li><strong>Vision-language DSL:</strong> Neural perception functions and symbolic operators form a
              probabilistic grammar.</li>
            <li><strong>Program search:</strong> Heap search finds the most accurate, most probable executable rule.
            </li>
          </ul>
        </div>
        <div class="column is-half">
          <h2 class="title is-4">Key findings</h2>
          <ul class="content">
            <li>Up to <strong>+13.5% balanced accuracy</strong> over raw VLM prompting.</li>
            <li>Small VLMs profit the most, retaining performance on out-of-distribution synthetic scenes.</li>
            <li>Structured programs stay consistent even when dataset labels are noisy.</li>
            <li>DSL edits enable knowledge insertion and shortcut mitigation.</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <section class="section is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Vision-Language Programs Pipeline</h2>
          <p>
            VLP cleanly splits perception from reasoning: a VLM supplies symbols, program synthesis handles logic, and
            programs execute directly on images. The PCFG prior steers search toward concise, high-accuracy rules that
            honor every positive and negative example.
          </p>
          <div class="hero-body">
            <img src="./static/images/method.webp" class="interpretability-image" alt="VLP pipeline overview" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-vcentered is-variable is-5">
        <div class="column">
          <h2 class="title is-3">Datasets &amp; setup</h2>
          <ul class="content">
            <li>6-shot <strong>Bongard-HOI</strong>, <strong>Bongard-OpenWorld</strong>, <strong>Bongard-RWR</strong>
              for real-world concept induction.</li>
            <li><strong>COCOLogic</strong> for real-world logical rules; <strong>CLEVR-Hans3</strong> for synthetic,
              compositional reasoning.</li>
            <li>Tested with InternVL3, Kimi-VL, Qwen2.5/3 VL, and GPT baselines.</li>
            <li>Heap search over a PCFG with depth up to 4–6; balanced accuracy on held-out queries.</li>
          </ul>
        </div>
        <div class="column">
          <h2 class="title is-4">Results snapshot</h2>
          <ul class="content">
            <li>Consistent improvements across all VLMs and datasets.</li>
            <li>Strongest gains on CLEVR-Hans3 where perception is uncertain.</li>
            <li>Thinking models + VLP retain accuracy while cutting token usage.</li>
            <li>Programs remain robust when labels are noisy, unlike free-form rules.</li>
          </ul>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Qualitative reasoning</h2>
          <p>On Bongard-RWR, direct VLM prompting hallucinated an “abundance” rule. VLP grounded objects, inferred the
            property <em>round</em>, and synthesized a concise executable program—correctly classifying all queries.</p>
          <div class="hero-body">
            <img src="./static/images/example_rwr.webp" class="interpretability-image"
              alt="VLP qualitative example on Bongard-RWR" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">BibTeX</h2>
      <pre><code>@article{wuest2025vlp,
  title={Synthesizing Visual Concepts as Vision-Language Programs},
  author={W{\"u}st, Antonia and Stammer, Wolfgang and Shindo, Hikaru and Dhami, Devendra Singh and Helff, Lukas and Kersting, Kristian},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/pdf/2511.18964" target="_blank">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/ml-research/vision-language-programs" target="_blank">
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>The website template is based on the source code of <a
                href="https://github.com/nerfies/nerfies.github.io">this website</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
